---
title: "Supporting assortment selection process for coffee shop"
author: "Alicja Komorniczak 260549, Adam Kawa≈Çko 262329"
date: "`r format(Sys.time(), '%m/%d/%Y %X')`"
output:
  rmdformats::readthedown:
    highlight: kate
    number_sections: true
    self_contained: true
---

```{r setup, include=FALSE}
## Global options
library(knitr)
opts_chunk$set(echo = TRUE, 
               cache = FALSE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE,
               fig.width = 7.4,
               fig.height = 4.7,
               fig.align = "center")
opts_knit$set(width = 80)
```

# Introduction

This report focuses on the analysis of a large dataset of coffee products, which includes a variety of specifications, ratings, and other characteristics related to the products. Our goal is to gain insights into the coffee market, examining factors such as price range, ratings, and the relationship between coffee specifications and their adjusted ratings. Through data cleaning, feature engineering, and statistical analysis, we aim to uncover trends and patterns in the dataset that can inform both consumers and producers of coffee.

The dataset contains several attributes, such as price, rating, number of opinions, flavor profiles (bitterness, acidity, sweetness, flavor intensity), and other product specifications like the coffee's origin, roaster, and packaging. Using this information, we perform a series of transformations and visualizations to provide a deeper understanding of how different factors contribute to coffee ratings and market pricing.   

# Data scraping

```{r}
library(httr)
library(rvest)
library(stringi)

# Fetch content of the webpage with a user-agent header
url <- "https://www.konesso.pl/pol_m_Kawa_Rodzaj_Kawa-ziarnista-2160.html"

# Instead of actual scraping, you can set a static response here
# response <- tryCatch(
#   GET(url, user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36")),
#   error = function(e) stop("Error during page fetch: ", e)
# )

# Check status of the response (now we skip this as no data will be fetched)
# if (status_code(response) != 200) stop("Failed to fetch page content")
# webpage <- tryCatch(
#   content(response, "text", encoding = "UTF-8") %>% read_html(),
#   error = function(e) stop("Error during page parsing: ", e)
# )

# Initialize lists to store data
# Using sample data instead of scraped content
product_containers <- list()  # No actual data to process
titles <- c("Sample Title 1", "Sample Title 2", "Sample Title 3", "Sample Title 4", "Sample Title 5")
descriptions <- c("Product description 1", "Product description 2", "Product description 3", "Product description 4", "Product description 5")
specifications_list <- list(
  c("Specification 1.1", "Specification 1.2"),
  c("Specification 2.1", "Specification 2.2"),
  c("Specification 3.1", "Specification 3.2"),
  c("Specification 4.1", "Specification 4.2"),
  c("Specification 5.1", "Specification 5.2")
)
prices <- c("39.99", "45.50", "50.00", "60.75", "55.00")
producers <- c("Producer 1", "Producer 2", "Producer 3", "Producer 4", "Producer 5")
ratings <- c(4.5, 4.7, 4.0, 4.3, 4.8)
reviews_counts <- c(100, 150, 200, 50, 120)

# Create a data frame from the first 5 products
num_products <- min(5, length(titles))
data <- data.frame(
  Title = titles[1:num_products],
  Description = descriptions[1:num_products],
  Price = prices[1:num_products]
)
data$Specifications <- specifications_list[1:num_products]

# Output the result with sample data
print(data)
```

# Data cleaning

```{r}
# Load necessary libraries
library(tidyverse)
library(stringi)
library(dplyr)
library(ggplot2)
library(reshape2)

# Read the CSV file with UTF-8 encoding
df_kawa <- read_csv2("kawa_with_bom_ok.csv", locale = locale(encoding = "UTF-8"))

# Define a function to process specifications from coffee data
process_specifications <- function(specifications_text) {
  specs <- stri_split_fixed(specifications_text, " | ", simplify = TRUE)
  spec_names <- stri_extract(specs, regex = "^[^:]+(?=:)")
  spec_values <- stri_extract(specs, regex = "(?<=:).+$")
  spec_list <- setNames(spec_values, spec_names)
  return(spec_list)
}

# Process specifications and create additional columns
specifications_processed <- lapply(df_kawa$Specifications, process_specifications)

# Convert the results into a data frame
specifications_df <- bind_rows(specifications_processed)

# Join these columns to the main data frame
df_kawa <- bind_cols(df_kawa, specifications_df)

# Remove the original 'Specifications' column
df_kawa <- df_kawa %>% select(-Specifications)

# Identify the number of non-NA values in each column
non_na_counts <- colSums(!is.na(df_kawa))

# Filter columns where the number of non-NA values is greater than or equal to 1000
df_kawa_filtered <- df_kawa[, non_na_counts >= 1000]

# Rename columns to English, with underscores instead of spaces and hyphens
colnames(df_kawa_filtered) <- c(
  "Title", "Description", "Price", "Producer", "Rating", "Number_of_reviews",
  "Composition", "Roasting_degree", "Caffeine_content", "Type", "Roastery", 
  "Purpose", "Packaging", "Preparation_method", "Crema", "Tastes", 
  "Flavor_intensity", "Bitterness", "Acidity", "Sweetness", 
  "Bean_origin", "Blend_or_Single", "Recommended_for", "Specialty_coffee"
)

# Replace spaces and hyphens with underscores in column names
colnames(df_kawa_filtered) <- gsub(" |-", "_", colnames(df_kawa_filtered))

# Function to convert values to a 1/5 to 5/5 scale
convert_to_5_scale <- function(value) {
  value <- gsub(" ", "", value)  # Remove white spaces
  if (grepl("Delicate", value)) {
    return(0.2)
  } else if (grepl("Medium", value)) {
    return(0.6)
  } else if (grepl("Strong", value)) {
    return(1)
  }
  
  # For numeric values like "2/5", "3/5"
  if (grepl("^\\d+/\\d+$", value)) {
    return(as.numeric(stri_extract(value, regex = "^\\d+")) / as.numeric(stri_extract(value, regex = "(?<=/).*")))
  }
  
  # For numeric values like "1", "2", "3", "4", "5"
  if (grepl("^\\d+$", value)) {
    if (as.numeric(value) == 1) {
      return(0.2)
    } else if (as.numeric(value) == 2) {
      return(0.4)
    } else if (as.numeric(value) == 3) {
      return(0.6)
    } else if (as.numeric(value) == 4) {
      return(0.8)
    } else {
      return(1)
    }
  }
  
  return(NA)
}

# Apply the function to convert to a 1/5 to 5/5 scale in relevant columns
df_kawa_filtered$Bitterness <- sapply(df_kawa_filtered$Bitterness, convert_to_5_scale)
df_kawa_filtered$Acidity <- sapply(df_kawa_filtered$Acidity, convert_to_5_scale)
df_kawa_filtered$Sweetness <- sapply(df_kawa_filtered$Sweetness, convert_to_5_scale)
df_kawa_filtered$Flavor_intensity <- sapply(df_kawa_filtered$Flavor_intensity, convert_to_5_scale)

# Remove the "names" attribute using unname()
df_kawa_filtered$Bitterness <- unname(df_kawa_filtered$Bitterness)
df_kawa_filtered$Acidity <- unname(df_kawa_filtered$Acidity)
df_kawa_filtered$Sweetness <- unname(df_kawa_filtered$Sweetness)
df_kawa_filtered$Flavor_intensity <- unname(df_kawa_filtered$Flavor_intensity)

# Divide the price by 100 and format it to two decimal places
df_kawa_filtered$Price <- as.numeric(df_kawa_filtered$Price) / 100
df_kawa_filtered$Price <- format(df_kawa_filtered$Price, nsmall = 2)

# Convert the 'Rating' column from character to numeric (float)
df_kawa_filtered$Rating <- as.numeric(df_kawa_filtered$Rating)

# Check the result after formatting
str(df_kawa_filtered)

df_kawa_filtered$Packaging <- ifelse(df_kawa_filtered$Packaging == " 1000", 
                                      " 1000g", 
                                      df_kawa_filtered$Packaging)

# Price Range Analysis

# Step 1: Standardize and clean the 'Packaging' column
df_kawa_filtered$Weight_g <- as.numeric(gsub("[^0-9]", "", df_kawa_filtered$Packaging))

# Handle specific cases
df_kawa_filtered$Weight_g[grepl("Glass bottle", df_kawa_filtered$Packaging)] <- NA  # Non-numeric, set to NA

# Step 2: Clean 'Price' and convert to numeric
df_kawa_filtered$Price <- as.numeric(gsub(",", ".", gsub("\\s", "", df_kawa_filtered$Price)))

# Step 3: Calculate "Price per Gram", excluding rows with NA in Weight_g or Price
df_kawa_filtered <- df_kawa_filtered[!is.na(df_kawa_filtered$Weight_g) & !is.na(df_kawa_filtered$Price), ]
df_kawa_filtered$Price_per_gram <- df_kawa_filtered$Price / df_kawa_filtered$Weight_g

# Step 4: Group coffees into price ranges
df_kawa_filtered$Price_range <- cut(
  df_kawa_filtered$Price_per_gram,
  breaks = c(0, 0.05, 0.1, 0.2, Inf),
  labels = c("Budget (< 0.05)", "Mid-range (0.05 - 0.1)", "Premium (0.1 - 0.2)", "Luxury (> 0.2)"),
  right = FALSE
)

# Adding Adjusted Ratings Column

# Step 1: Handle NA values in 'Rating' and 'Number_of_reviews'
df_kawa_filtered$Rating[is.na(df_kawa_filtered$Rating)] <- 0
df_kawa_filtered$Number_of_reviews[is.na(df_kawa_filtered$Number_of_reviews)] <- 0

# Step 2: Calculate the "Adjusted_rating"
df_kawa_filtered$Adjusted_rating <- df_kawa_filtered$Rating * sqrt(df_kawa_filtered$Number_of_reviews)

# Step 3: Preview the result
head(df_kawa_filtered[, c("Rating", "Number_of_reviews", "Adjusted_rating")])

head(df_kawa_filtered)

summary_stats <- df_kawa_filtered %>%
  select(Price, Rating, Number_of_reviews, Bitterness, Acidity, Sweetness, Flavor_intensity, Price_per_gram, Adjusted_rating) %>%
  summary()

# Save the dataframe to a CSV file
write.csv(df_kawa_filtered, "kawa_filtered.csv", row.names = FALSE)
```

# Additional statistics

```{r}
detailed_stats <- df_kawa_filtered %>%
  select(Price, Rating, Number_of_reviews, Bitterness, Acidity, Sweetness, Flavor_intensity, Price_per_gram, Adjusted_rating) %>%
  summarise(
    Mean_Price = mean(Price, na.rm = TRUE),
    SD_Price = sd(Price, na.rm = TRUE),
    Min_Price = min(Price, na.rm = TRUE),
    Max_Price = max(Price, na.rm = TRUE),
    
    Mean_Rating = mean(Rating, na.rm = TRUE),
    SD_Rating = sd(Rating, na.rm = TRUE),
    Min_Rating = min(Rating, na.rm = TRUE),
    Max_Rating = max(Rating, na.rm = TRUE),
    
    Mean_Number_of_reviews = mean(Number_of_reviews, na.rm = TRUE),
    SD_Number_of_reviews = sd(Number_of_reviews, na.rm = TRUE),
    Min_Number_of_reviews = min(Number_of_reviews, na.rm = TRUE),
    Max_Number_of_reviews = max(Number_of_reviews, na.rm = TRUE),
    
    Mean_Bitterness = mean(Bitterness, na.rm = TRUE),
    SD_Bitterness = sd(Bitterness, na.rm = TRUE),
    Min_Bitterness = min(Bitterness, na.rm = TRUE),
    Max_Bitterness = max(Bitterness, na.rm = TRUE),
    
    Mean_Acidity = mean(Acidity, na.rm = TRUE),
    SD_Acidity = sd(Acidity, na.rm = TRUE),
    Min_Acidity = min(Acidity, na.rm = TRUE),
    Max_Acidity = max(Acidity, na.rm = TRUE),
    
    Mean_Sweetness = mean(Sweetness, na.rm = TRUE),
    SD_Sweetness = sd(Sweetness, na.rm = TRUE),
    Min_Sweetness = min(Sweetness, na.rm = TRUE),
    Max_Sweetness = max(Sweetness, na.rm = TRUE),
    
    Mean_Flavor_intensity = mean(Flavor_intensity, na.rm = TRUE),
    SD_Flavor_intensity = sd(Flavor_intensity, na.rm = TRUE),
    Min_Flavor_intensity = min(Flavor_intensity, na.rm = TRUE),
    Max_Flavor_intensity = max(Flavor_intensity, na.rm = TRUE),
    
    Mean_Price_per_gram = mean(Price_per_gram, na.rm = TRUE),
    SD_Price_per_gram = sd(Price_per_gram, na.rm = TRUE),
    Min_Price_per_gram = min(Price_per_gram, na.rm = TRUE),
    Max_Price_per_gram = max(Price_per_gram, na.rm = TRUE),
    
    Mean_Adjusted_rating = mean(Adjusted_rating, na.rm = TRUE),
    SD_Adjusted_rating = sd(Adjusted_rating, na.rm = TRUE),
    Min_Adjusted_rating = min(Adjusted_rating, na.rm = TRUE),
    Max_Adjusted_rating = max(Adjusted_rating, na.rm = TRUE)
  )

# Display the statistics
print(summary_stats)
print(detailed_stats)
```

# Visualizations

```{r}
# Step 5: Visualize the distribution of products in each price range
ggplot(df_kawa_filtered, aes(x = Price_range)) +
  geom_bar(fill = "steelblue", color = "black") +
  labs(
    title = "Distribution of Products by Price Range",
    x = "Price Range (PLN per Gram)",
    y = "Number of Products"
  ) +
  theme_minimal()

# Analyzing Producers, Number of Opinions, and Ratings

# Step 1: Calculate the average number of ratings and adjusted ratings for each producer
producer_analysis <- df_kawa_filtered %>%
  group_by(Producer) %>%
  summarise(
    Avg_Num_Ratings = mean(Number_of_reviews, na.rm = TRUE),
    Avg_Adjusted_Rating = mean(Adjusted_rating, na.rm = TRUE),
    Count_Products = n()
  ) %>%
  arrange(desc(Avg_Num_Ratings))  # Sort by popularity

# Step 2: Top producers by average number of ratings
top_popular_producers <- producer_analysis %>%
  top_n(10, Avg_Num_Ratings)

# Step 3: Top producers by adjusted rating
top_adjusted_rating_producers <- producer_analysis %>%
  top_n(10, Avg_Adjusted_Rating)

# Step 4: Visualize popularity
ggplot(top_popular_producers, aes(x = reorder(Producer, Avg_Num_Ratings), y = Avg_Num_Ratings)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top Producers by Average Number of Ratings",
    x = "Producer",
    y = "Average Number of Ratings"
  ) +
  theme_minimal()

ggplot(top_adjusted_rating_producers, aes(x = reorder(Producer, Avg_Adjusted_Rating), y = Avg_Adjusted_Rating)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  coord_flip() +
  labs(
    title = "Top Producers by Average Adjusted Rating",
    x = "Producer",
    y = "Average Adjusted Rating"
  ) +
  theme_minimal()

# Correlation Analysis of Coffee Specifications and Adjusted Rating

# Step 1: Prepare data for correlation analysis
correlation_data <- df_kawa_filtered %>%
  select(Acidity, Bitterness, Sweetness, Flavor_intensity, Adjusted_rating) %>%
  na.omit()  # Remove rows with NA values

# Step 2: Compute the correlation matrix
correlation_matrix <- cor(correlation_data)

# Convert matrix to long format for ggplot
correlation_long <- melt(correlation_matrix)

# Plot heatmap
ggplot(correlation_long, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1), space = "Lab", name = "Correlation") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text = element_text(size = 10)
  ) +
  labs(
    title = "Correlation Heatmap",
    x = "Specifications",
    y = "Specifications"
  )

# Distribution of Coffee Intensity (Flavor)
ggplot(df_kawa_filtered, aes(x = Flavor_intensity)) +
  geom_histogram(binwidth = 0.1, fill = "lightblue", color = "black") +
  labs(
    title = "Distribution of Coffee Intensity (Flavor)",
    x = "Flavor Intensity (Scale 1-5)",
    y = "Number of Products"
  ) +
  theme_minimal()

# Coffee Price vs. Rating
ggplot(df_kawa_filtered, aes(x = Price, y = Rating)) +
  geom_point(color = "steelblue", alpha = 0.6) +
  labs(
    title = "Coffee Price vs. Rating",
    x = "Price (PLN)",
    y = "Rating"
  ) +
  theme_minimal()
```

# Summary

In conclusion, this analysis provides valuable insights into the coffee market based on the available dataset. The price range analysis highlights how coffee prices vary per gram, revealing distinct categories from budget to luxury products. The relationship between coffee specifications and ratings shows how flavor profiles such as bitterness, acidity, and sweetness influence the overall coffee experience, with adjusted ratings offering a more nuanced evaluation that accounts for the number of opinions.

By analyzing producers' data, we identified key players with the highest number of reviews and those with the highest average ratings. Visualizations of these trends help identify market leaders and trends that can guide consumer preferences. Moreover, the correlation analysis between various flavor characteristics and adjusted ratings uncovers relationships that may be critical for consumers seeking specific taste profiles.

Overall, this report provides an in-depth exploration of coffee products, their ratings, and price ranges. The findings can serve as useful insights for consumers making purchasing decisions, as well as for producers looking to optimize their products and marketing strategies. Future work may involve expanding the analysis to include more granular data or incorporating consumer feedback to refine these findings further.

