---
title: "Supporting assortment selection process for coffee shop"
author: "Alicja Komorniczak 260549, Adam Kawałko 262329"
date: "`r format(Sys.time(), '%m/%d/%Y %X')`"
output:
  rmdformats::readthedown:
    highlight: kate
    number_sections: true
    self_contained: true
---

```{r setup, include=FALSE}
## Global options
library(knitr)
opts_chunk$set(echo = TRUE, 
               cache = FALSE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE,
               fig.width = 7.4,
               fig.height = 4.7,
               fig.align = "center")
opts_knit$set(width = 80)
```

# Introduction

This report focuses on the analysis of a large dataset of coffee products, which includes a variety of specifications, ratings, and other characteristics related to the products. Our goal is to gain insights into the coffee market, examining factors such as price range, ratings, and the relationship between coffee specifications and their adjusted ratings. Through data cleaning and statistical analysis, we aim to uncover trends and patterns in the dataset that can help understand trends and dependencies in coffee market.

The information we got from konesso website contains several attributes, such as price, rating, number of opinions, flavor profiles (bitterness, acidity, sweetness, flavor intensity), and other product specifications like the coffee's origin, roaster, and packaging. Using this information, we perform a series of transformations and visualizations to provide a deeper understanding of how different factors contribute to coffee ratings and market pricing.   

# Data scraping

In order to obtain dataset with information from konesso website with all necessery attributes for analysis, we scrape these information using appropiate code in R.
From each coffee bean product on konesso we scraped the following information: 

1. Product name,
2. Product description,
3. Specifications (such as degree of smoking, acidity etc.),
4. Price,
5. Producer,
6. Rating,
7. Number of reviews.

These were the key attributes targeted for extraction during this project.

## Main core of scraper

All necessary libraries were imported at the beginning of the script to handle `HTTP` requests, parse `HTML` content, process text data, and handle potential errors during the scraping process.

```{r}
library(httr)
library(rvest)
library(stringi)
```
The base URL of the webpage was defined, and an `HTTP GET` request was sent to access the page content while using a user-agent header.
```{r}
# Fetch content of the webpage with a user-agent header
url <- "https://www.konesso.pl/pol_m_Kawa_Rodzaj_Kawa-ziarnista-2160.html"


# response <- tryCatch(
#   GET(url, user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36")),
#   error = function(e) stop("Error during page fetch: ", e)
# )
```

Once the page's content was retrieved successfully, it was parsed into an `HTML` object using the `rvest` package, and product containers were extracted for further processing. 

```{r}
# Check status of the response (now we skip this as no data will be fetched)
# if (status_code(response) != 200) stop("Failed to fetch page content")
# webpage <- tryCatch(
#   content(response, "text", encoding = "UTF-8") %>% read_html(),
#   error = function(e) stop("Error during page parsing: ", e)
# )
```

## Data Extraction

The scraper iteratively processed each product container to extract specific attributes. For each attribute, list was initialized to store them. A delay of two seconds `(Sys.sleep(1))` was implemented between each iteration to avoid overwhelming the server and potential blocking.

For the purpose of example, the code below contains sample data instead of scarped content. The full scaring code is available in the attachment file `scraper.R`.

```{r}

# Initialize lists to store data
# Using sample data instead of scraped content
product_containers <- list()  # No actual data to process
titles <- c("Sample Title 1", "Sample Title 2", "Sample Title 3", "Sample Title 4", "Sample Title 5")
descriptions <- c("Product description 1", "Product description 2", "Product description 3", "Product description 4", "Product description 5")
specifications_list <- list(
  c("Specification 1.1", "Specification 1.2"),
  c("Specification 2.1", "Specification 2.2"),
  c("Specification 3.1", "Specification 3.2"),
  c("Specification 4.1", "Specification 4.2"),
  c("Specification 5.1", "Specification 5.2")
)
prices <- c("39.99", "45.50", "50.00", "60.75", "55.00")
producers <- c("Producer 1", "Producer 2", "Producer 3", "Producer 4", "Producer 5")
ratings <- c(4.5, 4.7, 4.0, 4.3, 4.8)
reviews_counts <- c(100, 150, 200, 50, 120)

# Create a data frame from the first 5 products
num_products <- min(5, length(titles))
data <- data.frame(
  Title = titles[1:num_products],
  Description = descriptions[1:num_products],
  Price = prices[1:num_products]
)
data$Specifications <- specifications_list[1:num_products]

```

Scraped data was saved in dataframe and later extracted into `CSV` file: `kawa_with_bom_ok.CSV`.


# Data cleaning

At the begining of data cleaning, the used libraries are loaded.
```{r}
# Load necessary libraries
library(tidyverse)
library(stringi)
library(dplyr)
library(ggplot2)
library(reshape2)
```

Previously saved dataframe with scraped data was imported into `df_kawa` dataframe in R with `UTF-8` encoding using `read_csv2`.

```{r}
df_kawa <- read_csv2("kawa_with_bom_ok.csv", locale = locale(encoding = "UTF-8"))
```

Next, a function `process_specifications` is defined to split and parse the specifications text into key-value pairs.
The function:

1. Splits the text by " | ".
2. Extracts keys (before :) and values (after :).
3. Returns a named list.

This function is applied to the Specifications column in the dataset. This is because while importing specifications such as bitterness, acidity etc. into a single dataframe columns, they were divided by " | ".


```{r}
# Define a function to process specifications from coffee data
process_specifications <- function(specifications_text) {
  specs <- stri_split_fixed(specifications_text, " | ", simplify = TRUE)
  spec_names <- stri_extract(specs, regex = "^[^:]+(?=:)")
  spec_values <- stri_extract(specs, regex = "(?<=:).+$")
  spec_list <- setNames(spec_values, spec_names)
  return(spec_list)
}
```

Processed specifications are used for creating new columns and original `Specifications` column is removed from dataframe. Then the columns where there is more than 1000 empty values are removed due to being unnecessery for further analysis. Last step in segment below is used for changing column names.


```{r}
specifications_processed <- lapply(df_kawa$Specifications, process_specifications)
specifications_df <- bind_rows(specifications_processed)
df_kawa <- bind_cols(df_kawa, specifications_df)
df_kawa <- df_kawa %>% select(-Specifications)

# Identify the number of non-NA values in each column
non_na_counts <- colSums(!is.na(df_kawa))

# Filter columns where the number of non-NA values is greater than or equal to 1000
df_kawa_filtered <- df_kawa[, non_na_counts >= 1000]

# Rename columns, with underscores instead of spaces and hyphens
colnames(df_kawa_filtered) <- c(
  "Tytuł", "Opis", "Cena", "Producent", "Ocena", "Liczba_opinii",
  "Skład", "Stopień_palenia", "Zawartość_kofeiny", "Rodzaj", "Palarnia", 
  "Przeznaczenie", "Opakowanie", "Sposób_przygotowania", "Crema", "Wyczuwalne_smaki", 
  "Intensywność_smaku", "Gorycz", "Kwasowość", "Słodycz", 
  "Pochodzenie_ziaren", "Blend_czy_Single", "Polecana_do", "Kawa_Specialty"
)

# Replace spaces and hyphens with underscores in column names
colnames(df_kawa_filtered) <- gsub(" |-", "_", colnames(df_kawa_filtered))
```

The trouble we came across while initially analysing data from the website, mainy specifications, is that there are various scales used for describing coffee flavors and characteristics. In some cases, there were levels explained by words (like "weak", "medium", "strong") and sometimes the 1-5 scale was used. 
A function `convert_to_5_scale` standardizes subjective or numeric ratings (like "2/5" or "Medium") into a scale from 0.2 to 1:

1. "Delicate" → 0.2
2. "Medium" → 0.6
3. "Strong" → 1

Numerical fractions (e.g., "2/5") are converted to decimals.
This is applied to columns such as `Bitterness`, `Acidity`, `Sweetness`, and `Flavor_intensity`.

```{r}
# Function to convert values to a 1/5 to 5/5 scale
convert_to_5_scale <- function(value) {
  value <- gsub(" ", "", value)  # Remove white spaces
  if (grepl("Delicate", value)) {
    return(0.2)
  } else if (grepl("Medium", value)) {
    return(0.6)
  } else if (grepl("Strong", value)) {
    return(1)
  }
  
  # For numeric values like "2/5", "3/5"
  if (grepl("^\\d+/\\d+$", value)) {
    return(as.numeric(stri_extract(value, regex = "^\\d+")) / as.numeric(stri_extract(value, regex = "(?<=/).*")))
  }
  
  # For numeric values like "1", "2", "3", "4", "5"
  if (grepl("^\\d+$", value)) {
    if (as.numeric(value) == 1) {
      return(0.2)
    } else if (as.numeric(value) == 2) {
      return(0.4)
    } else if (as.numeric(value) == 3) {
      return(0.6)
    } else if (as.numeric(value) == 4) {
      return(0.8)
    } else {
      return(1)
    }
  }
  
  return(NA)
}
```
Several other changes were made that involved either transforming the existing data or creating new column:

1. Dividing the price by 100 and format it to two decimal places,
2. Calculating price per gram,
3. The products were divided into price ranges and were assigned appropriate label based on price per gram.

Cleaned and filtered data was then saved into `kawa_filtered.csv` file. Full file with data cleaning can be found in attached file `cleaner.R`.


# Analysis

```{r}
# Loading libraries
library(tidyverse)
library(ggplot2)

# Reading data from "kawa_filtered.csv"
df_kawa_filtered <- read.csv("kawa_filtered.csv")

# General statistics
summary_stats <- df_kawa_filtered %>%
  select(Cena, Ocena, Liczba_opinii, Gorycz, Kwasowość, Słodycz, Intensywność_smaku, Cena_za_gram, Skorygowana_ocena) %>%
  summary()

# Additional statistics: mean, standard deviation, min, max
detailed_stats <- df_kawa_filtered %>%
  select(Cena, Ocena, Liczba_opinii, Gorycz, Kwasowość, Słodycz, Intensywność_smaku, Cena_za_gram, Skorygowana_ocena) %>%
  summarise(
    Mean_Cena = mean(Cena, na.rm = TRUE),
    SD_Cena = sd(Cena, na.rm = TRUE),
    Min_Cena = min(Cena, na.rm = TRUE),
    Max_Cena = max(Cena, na.rm = TRUE),
    
    Mean_Ocena = mean(Ocena, na.rm = TRUE),
    SD_Ocena = sd(Ocena, na.rm = TRUE),
    Min_Ocena = min(Ocena, na.rm = TRUE),
    Max_Ocena = max(Ocena, na.rm = TRUE),
    
    Mean_Liczba_opinii = mean(Liczba_opinii, na.rm = TRUE),
    SD_Liczba_opinii = sd(Liczba_opinii, na.rm = TRUE),
    Min_Liczba_opinii = min(Liczba_opinii, na.rm = TRUE),
    Max_Liczba_opinii = max(Liczba_opinii, na.rm = TRUE),
    
    Mean_Gorycz = mean(Gorycz, na.rm = TRUE),
    SD_Gorycz = sd(Gorycz, na.rm = TRUE),
    Min_Gorycz = min(Gorycz, na.rm = TRUE),
    Max_Gorycz = max(Gorycz, na.rm = TRUE),
    
    Mean_Kwasowość = mean(Kwasowość, na.rm = TRUE),
    SD_Kwasowość = sd(Kwasowość, na.rm = TRUE),
    Min_Kwasowość = min(Kwasowość, na.rm = TRUE),
    Max_Kwasowość = max(Kwasowość, na.rm = TRUE),
    
    Mean_Słodycz = mean(Słodycz, na.rm = TRUE),
    SD_Słodycz = sd(Słodycz, na.rm = TRUE),
    Min_Słodycz = min(Słodycz, na.rm = TRUE),
    Max_Słodycz = max(Słodycz, na.rm = TRUE),
    
    Mean_Intensywność_smaku = mean(Intensywność_smaku, na.rm = TRUE),
    SD_Intensywność_smaku = sd(Intensywność_smaku, na.rm = TRUE),
    Min_Intensywność_smaku = min(Intensywność_smaku, na.rm = TRUE),
    Max_Intensywność_smaku = max(Intensywność_smaku, na.rm = TRUE),
    
    Mean_Cena_za_gram = mean(Cena_za_gram, na.rm = TRUE),
    SD_Cena_za_gram = sd(Cena_za_gram, na.rm = TRUE),
    Min_Cena_za_gram = min(Cena_za_gram, na.rm = TRUE),
    Max_Cena_za_gram = max(Cena_za_gram, na.rm = TRUE)
  )

# Plotting distribution of price per gram
ggplot(df_kawa_filtered, aes(x = Cena_za_gram)) +
  geom_histogram(binwidth = 0.01, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Price per Gram", x = "Price per Gram", y = "Number of Coffees")

# Scatter plot of price vs rating
ggplot(df_kawa_filtered, aes(x = Cena, y = Ocena)) +
  geom_point(color = "blue", alpha = 0.5) +
  labs(title = "Price vs Rating", x = "Price", y = "Rating")

# Boxplot by price range
ggplot(df_kawa_filtered, aes(x = Przedział_cenowy, y = Ocena)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Rating by Price Range", x = "Price Range", y = "Rating")

# Identify Most Popular Producers
# Count reviews per producer and select top 5
popular_producers <- df_kawa_filtered %>%
  group_by(Producent) %>%
  summarise(Total_Reviews = sum(Liczba_opinii, na.rm = TRUE)) %>%
  arrange(desc(Total_Reviews)) %>%
  head(5)

# Bar plot of most popular producers
ggplot(popular_producers, aes(x = reorder(Producent, -Total_Reviews), y = Total_Reviews)) +
  geom_bar(stat = "identity", fill = "dodgerblue") +
  coord_flip() +
  labs(title = "Top 5 Most Popular Producers by Total Reviews", x = "Producer", y = "Total Reviews")

#Identify Producers with Cheapest Products
# Average price per producer
cheapest_producers <- df_kawa_filtered %>%
  group_by(Producent) %>%
  summarise(Average_Price = mean(Cena_za_gram, na.rm = TRUE)) %>%
  arrange(Average_Price) %>%
  head(5)

# Bar plot for producers with cheapest products
ggplot(cheapest_producers, aes(x = reorder(Producent, Average_Price), y = Average_Price)) +
  geom_bar(stat = "identity", fill = "lightcoral") +
  coord_flip() +
  labs(title = "Top 5 Producers with Cheapest Products", x = "Producer", y = "Average Price per Gram")

#Producers with Best Reviews and most reviews
# Calculate average rating and total reviews per producer
producers_stats <- df_kawa_filtered %>%
  group_by(Producent) %>%
  summarise(
    Average_Rating = mean(Ocena, na.rm = TRUE),
    Total_Reviews = sum(Liczba_opinii, na.rm = TRUE)
  ) %>%
  arrange(desc(Average_Rating))

# Filter for the top producers (e.g., top 5 by average rating)
top_producers_stats <- producers_stats %>%
  head(10)

# Bar plot showing both average rating and total reviews with labels
ggplot(top_producers_stats, aes(x = reorder(Producent, -Average_Rating))) +
  geom_bar(aes(y = Average_Rating), stat = "identity", fill = "gold", alpha = 0.7) +
  geom_point(aes(y = Total_Reviews / 10), color = "blue", size = 3) + # Adjust scaling for clarity
  geom_line(aes(y = Total_Reviews / 10, group = 1), color = "blue", linetype = "dashed") +
  geom_text(aes(y = Total_Reviews / 10, label = Average_Rating), vjust = -0.5, color = "blue", size = 4) + # Add labels for reviews
  labs(
    title = "Top Producers: Rating and Number of Reviews",
    x = "Producer",
    y = "Average Rating (Bar) / Total Reviews (Scaled by 1/10)",
  ) +
  theme_minimal() +
  coord_flip()

#Analyze Review Counts and Ratings by Flavor Characteristics
# Correlation of characteristics with reviews and ratings
characteristics_correlation <- df_kawa_filtered %>%
  select(Gorycz, Kwasowość, Słodycz, Intensywność_smaku, Ocena, Liczba_opinii) %>%
  cor(use = "complete.obs")

# Heatmap of correlation
library(reshape2)
melted_correlation <- melt(characteristics_correlation)
ggplot(melted_correlation, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "red", mid = "white", high = "blue", midpoint = 0) +
  labs(title = "Correlation Heatmap", x = "Variable", y = "Variable")

```

# Summary

In conclusion, this analysis provides valuable insights into the coffee market based on the available dataset. The price range analysis highlights how coffee prices vary per gram, revealing distinct categories from budget to luxury products. The relationship between coffee specifications and ratings shows how flavor profiles such as bitterness, acidity, and sweetness influence the overall coffee experience, with adjusted ratings offering a more nuanced evaluation that accounts for the number of opinions.

By analyzing producers' data, we identified key players with the highest number of reviews and those with the highest average ratings. Visualizations of these trends help identify market leaders and trends that can guide consumer preferences. Moreover, the correlation analysis between various flavor characteristics and adjusted ratings uncovers relationships that may be critical for consumers seeking specific taste profiles.

Overall, this report provides an in-depth exploration of coffee products, their ratings, and price ranges. The findings can serve as useful insights for consumers making purchasing decisions, as well as for producers looking to optimize their products and marketing strategies. Future work may involve expanding the analysis to include more granular data or incorporating consumer feedback to refine these findings further.

